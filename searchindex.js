Search.setIndex({"docnames": ["._index", "._what_is_xformers", "components/._attentions", "components/._feedforward", "components/._index", "components/._mha", "components/._ops", "components/._position_embedding", "components/._reversible", "components/attentions", "components/feedforward", "components/index", "components/mha", "components/ops", "components/position_embedding", "components/reversible", "custom_parts/._index", "custom_parts/index", "index", "tutorials/._blocksparse", "tutorials/._extend_attentions", "tutorials/._index", "tutorials/._reversible", "tutorials/._sparse_vit", "tutorials/._triton", "tutorials/._use_attention", "tutorials/blocksparse", "tutorials/extend_attentions", "tutorials/index", "tutorials/reversible", "tutorials/sparse_vit", "tutorials/triton", "tutorials/use_attention", "what_is_xformers"], "filenames": ["._index.rst", "._what_is_xformers.rst", "components/._attentions.rst", "components/._feedforward.rst", "components/._index.rst", "components/._mha.rst", "components/._ops.rst", "components/._position_embedding.rst", "components/._reversible.rst", "components/attentions.rst", "components/feedforward.rst", "components/index.rst", "components/mha.rst", "components/ops.rst", "components/position_embedding.rst", "components/reversible.rst", "custom_parts/._index.rst", "custom_parts/index.rst", "index.rst", "tutorials/._blocksparse.rst", "tutorials/._extend_attentions.rst", "tutorials/._index.rst", "tutorials/._reversible.rst", "tutorials/._sparse_vit.rst", "tutorials/._triton.rst", "tutorials/._use_attention.rst", "tutorials/blocksparse.rst", "tutorials/extend_attentions.rst", "tutorials/index.rst", "tutorials/reversible.rst", "tutorials/sparse_vit.rst", "tutorials/triton.rst", "tutorials/use_attention.rst", "what_is_xformers.rst"], "titles": ["&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Attention mechanisms", "Feedforward mechanisms", "API Reference", "Multi Head Attention", "xFormers optimized operators", "Position Embeddings", "Reversible layer", "&lt;no title&gt;", "Custom parts reference", "Welcome to xFormers\u2019s documentation!", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Using BlockSparseAttention", "Extend the xFormers parts zoo", "Tutorials", "Using the Reversible block", "Replace all attentions from an existing ViT model with a sparse equivalent?", "Using Triton-based layers", "I\u2019m only interested in testing out the attention mechanisms that are hosted here", "What is xFormers?"], "terms": {"mac": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "os": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "x": [0, 1, 2, 3, 4, 5, 6, 7, 8, 13, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 29, 31, 32], "2": [0, 1, 2, 3, 4, 5, 6, 7, 8, 13, 16, 19, 20, 21, 22, 23, 24, 25, 26, 29], "rtta": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "\u025a": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "com": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 17, 19, 20, 21, 22, 23, 24, 25], "appl": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "quarantine0081": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "66106dcb": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "sharingd": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "35af41b9": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "643f": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "4885": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "ba06": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "adf51159fb53": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "thi": [0, 1, 2, 3, 4, 5, 6, 7, 8, 13, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33], "resourc": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "fork": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25, 27], "intention": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "left": [0, 1, 2, 3, 4, 5, 6, 7, 8, 13, 16, 19, 20, 21, 22, 23, 24, 25], "blank": [0, 1, 2, 3, 4, 5, 6, 7, 8, 16, 19, 20, 21, 22, 23, 24, 25], "xformer": [11, 17, 26, 28, 29, 30, 31, 32], "optim": [11, 18, 31, 33], "oper": [11, 18, 31], "memori": [11, 26, 29], "effici": [11, 29], "attent": [11, 17, 18, 26, 27, 28, 29], "mechan": [11, 13, 18, 27, 28, 30], "feedforward": [11, 18, 29], "posit": [11, 13, 18], "embed": [11, 13, 18], "revers": [11, 18, 28], "layer": [11, 18, 28, 29], "multi": [11, 13, 18, 29, 32], "head": [11, 13, 18, 26, 29, 32], "class": [13, 27, 29], "op": 13, "attentionbia": 13, "sourc": [13, 17, 30, 33], "base": [13, 17, 18, 27, 28, 30, 32], "object": 13, "custom": 13, "bia": 13, "can": [13, 17, 18, 26, 27, 29, 30, 31, 32, 33], "appli": [13, 27, 29], "attn_bia": 13, "argument": [13, 27], "memory_efficient_attent": 13, "That": [13, 32], "function": [13, 26, 29, 30], "ha": 13, "abil": 13, "add": 13, "tensor": [13, 27, 29, 30, 31], "qk": 13, "t": 13, "matrix": [13, 26], "befor": 13, "us": [13, 17, 18, 27, 28, 30, 32], "softmax": 13, "part": [13, 28], "calcul": 13, "The": [13, 17, 26, 27, 29, 30, 31, 32], "shape": [13, 30], "b": [13, 29], "1": [13, 26, 29, 30, 31, 32], "n_queri": 13, "number": [13, 29, 31], "kei": [13, 26], "given": [13, 27, 30, 32, 33], "input": [13, 29, 32], "most": 13, "common": [13, 17], "case": [13, 30, 31, 32], "an": [13, 18, 26, 28, 31, 32], "contain": 13, "onli": [13, 17, 18, 28, 31], "zero": 13, "neg": 13, "infin": 13, "which": [13, 17, 18, 26, 27, 29, 30, 31, 32, 33], "form": 13, "mask": [13, 17, 26, 30, 32], "so": [13, 26, 27, 31, 33], "some": [13, 17, 26, 29, 30, 31, 32], "queri": [13, 26], "attend": 13, "children": 13, "defin": [13, 26, 27, 29, 30, 33], "altern": [13, 30, 32], "thing": [13, 27], "when": [13, 17, 29, 30, 31], "instead": 13, "torch": [13, 26, 27, 29, 30, 31, 32], "doe": [13, 29], "need": [13, 17, 26, 27, 29], "materi": 13, "hardcod": 13, "kernel": [13, 18, 31], "better": [13, 26, 31], "perform": [13, 31], "see": [13, 17, 29], "fmha": 13, "lowertriangularmask": 13, "lowertriangularfrombottomrightmask": 13, "lowertriangularmaskwithtensorbia": 13, "blockdiagonalmask": 13, "blockdiagonalcausalmask": 13, "tupl": 13, "int": [13, 27, 29], "dtype": [13, 26], "float32": 13, "devic": [13, 26, 32], "union": [13, 29], "str": 13, "cpu": 13, "veri": [13, 29, 31, 32], "slow": [13, 17], "we": [13, 17, 26, 27, 29, 30, 32], "don": 13, "attempt": 13, "make": [13, 26, 29, 31, 32], "fast": [13, 26, 33], "debug": 13, "test": [13, 18, 27, 28, 33], "should": [13, 29, 32], "like": [13, 27, 29, 30, 31, 32], "q_seqlen": 13, "k_seqlen": 13, "attentionopbas": 13, "baseoper": 13, "ani": [13, 17, 27, 30, 31, 33], "cutlass": 13, "fwop": 13, "bwop": 13, "flash": 13, "triton": [13, 18, 26, 28], "small_k": 13, "classmethod": 13, "not_supported_reason": 13, "d": 13, "list": 13, "return": [13, 30], "reason": 13, "why": 13, "support": [13, 17, 30, 31], "run": [13, 26, 31], "empti": [13, 26], "valu": [13, 17, 26, 33], "option": [13, 18, 27, 29], "none": [13, 27, 31], "p": 13, "float": [13, 27], "0": [13, 17, 26, 32], "scale": [13, 17], "type": [13, 27], "attentionfwopbas": 13, "attentionbwopbas": 13, "output_dtyp": 13, "follow": [13, 17, 27, 29, 30, 31, 33], "self": [13, 26, 27, 29], "Not": 13, "o": 13, "n": [13, 27, 29], "must": 13, "format": 13, "m": [13, 18, 28, 29], "h": 13, "k": [13, 27], "where": [13, 26], "batch": [13, 26, 27, 32], "size": [13, 26], "sequenc": [13, 26, 29, 31, 32], "length": [13, 26], "per": [13, 26], "If": [13, 26], "have": [13, 26, 30], "dimens": [13, 26, 31, 32], "3": [13, 17, 29, 30, 32], "assum": 13, "ar": [13, 17, 18, 27, 28, 29, 30, 31, 33], "also": [13, 17, 27, 29, 31, 33], "5": 13, "gqa": 13, "note": [13, 26, 29, 30], "below": 13, "contigu": 13, "requir": [13, 26, 27, 29], "last": [13, 31], "s": [13, 26, 27, 30, 31, 32], "stride": 13, "equival": [13, 18, 28], "pytorch": [13, 18, 26, 30, 31], "code": [13, 17, 27, 31, 32], "transpos": 13, "attn": 13, "f": [13, 26, 29], "dropout": [13, 26, 27, 32], "exampl": [13, 17, 26, 30, 31], "import": [13, 26, 30, 31, 32], "xop": 13, "comput": [13, 17, 26, 29], "regular": 13, "y": [13, 31], "q": [13, 27], "v": [13, 27], "With": 13, "causal": [13, 26, 27], "hardwar": 13, "nvidia": [13, 31], "gpu": [13, 17, 29, 31], "capabl": [13, 17, 31], "abov": [13, 17, 29, 30], "6": [13, 17, 26, 27], "p100": 13, "datatyp": 13, "f16": 13, "bf16": 13, "f32": 13, "experiment": 13, "mqa": 13, "group": 13, "featur": 13, "forward": [13, 26, 27, 29], "pass": [13, 26, 29], "you": [13, 17, 26, 27, 29, 30, 31, 32, 33], "16": [13, 26, 30, 32], "provid": [13, 17, 26, 31, 32], "dim": [13, 29, 30], "g": [13, 29], "here": [13, 18, 26, 28], "8": [13, 17, 26, 30], "pleas": [13, 26, 29, 30], "automat": [13, 17, 27, 33], "broadcast": 13, "manual": [13, 30], "call": [13, 27], "32": [13, 26], "128": 13, "kwarg": [13, 26, 27, 29], "dict": [13, 32], "cuda": [13, 18, 26, 30, 31], "float16": [13, 26], "randn": [13, 26], "out_gqa": 13, "reshap": 13, "4": 13, "expand": 13, "rais": 13, "notimplementederror": 13, "mha": [13, 29], "valueerror": 13, "invalid": 13, "paramet": [13, 30, 32], "mq": 13, "mkv": 13, "kv": 13, "default": [13, 17, 27], "For": 13, "arbitrari": 13, "slower": 13, "probabl": [13, 26], "disabl": 13, "set": [13, 17, 29], "factor": 13, "recommend": [13, 27], "dispatch": [13, 32], "best": 13, "depend": [13, 32], "larg": [13, 29], "includ": [13, 29], "without": [13, 29], "tensorcor": 13, "old": 13, "sm60": 13, "small": [13, 26], "pre": 13, "amper": 13, "bmk": 13, "extra": [13, 26, 27, 29, 30], "might": 13, "done": [13, 27], "deprec": 13, "new": [13, 27, 32, 33], "ck": [13, 27], "compos": [13, 33], "ck_decod": 13, "256": 13, "fit": 13, "regist": [13, 27], "work": [13, 27, 30, 32], "mi250x": 13, "localattentionfrombottomrightmask": 13, "window_left": 13, "window_right": 13, "A": [13, 17, 26, 29], "local": [13, 27, 31, 33], "window": 13, "_left": 13, "_right": 13, "num": 13, "_queri": 13, "_kei": 13, "from": [13, 17, 18, 26, 27, 28, 29, 31, 32], "print": [13, 26], "exp": 13, "4x4": 13, "4x5": 13, "illustr": 13, "total": 13, "tensor_arg": 13, "tensor_kwarg": 13, "lower": [13, 26], "triangular": [13, 26], "aka": 13, "cannot": [13, 33], "farther": 13, "initi": [13, 32], "than": [13, 17, 26, 29, 31], "equal": 13, "add_bia": 13, "creat": [13, 17, 18], "exactli": [13, 29], "same": [13, 27, 29], "differ": [13, 26, 27, 29], "shift": 13, "In": [13, 30, 31, 32], "other": [13, 27, 29, 30, 31, 33], "word": 13, "nearer": 13, "final": 13, "between": [13, 26, 29], "right": 13, "thei": [13, 17, 18], "becom": [13, 29, 32], "make_local_attent": 13, "window_s": 13, "lowertriangularfrombottomrightlocalattentionmask": 13, "combin": [13, 18, 29, 32], "_window_s": 13, "both": [13, 27], "whose": 13, "distanc": 13, "either": [13, 31, 32], "less": [13, 17, 29], "i": [13, 18, 28], "e": [13, 17], "greater": 13, "green": 13, "area": 13, "grei": 13, "out": [13, 18, 26, 28], "addit": 13, "q_seqinfo": 13, "_seqleninfo": 13, "k_seqinfo": 13, "_batch_siz": 13, "block": [13, 17, 18, 26, 27, 28, 33], "diagon": 13, "each": [13, 31, 33], "divid": 13, "handl": 13, "via": [13, 29], "from_tensor_list": 13, "list_x": 13, "linear": 13, "nn": [13, 26, 27, 29, 30, 31], "unbind": 13, "list_out": 13, "split": 13, "assert": 13, "from_seqlen": 13, "kv_seqlen": 13, "concaten": 13, "back": 13, "vari": 13, "m_i": 13, "all": [13, 18, 27, 28, 29, 31, 32, 33], "correspond": [13, 31], "along": [13, 26, 31], "sum_i": 13, "invers": 13, "token": 13, "possibl": [13, 26, 31, 32], "make_caus": 13, "make_causal_from_bottomright": 13, "blockdiagonalcausalfrombottomrightmask": 13, "prefix": 13, "blockdiagonalcausallocalattentionmask": 13, "make_local_attention_from_bottomright": 13, "blockdiagonalcausallocalattentionfrombottomrightmask": 13, "start": [13, 26, 31], "bottom": 13, "except": 13, "nor": 13, "one": [13, 26, 27, 29, 30, 33], "allow": [13, 32, 33], "num_kei": 13, "num_queri": 13, "otherwis": 13, "vector": 13, "inf": 13, "blockdiagonalpaddedkeysmask": 13, "_paddedseqleninfo": 13, "pad": 13, "space": 13, "12": 13, "three": [13, 27], "max": 13, "want": [13, 17, 27, 30], "first": [13, 29], "kv_pad": 13, "causal_diagon": 13, "upperbound": 13, "individu": 13, "unus": 13, "bc": 13, "blockdiagonalcausalwithoffsetpaddedkeysmask": 13, "offset": 13, "pagedblockdiagonalpaddedkeysmask": 13, "block_tabl": 13, "page_s": 13, "page": 13, "batch_siz": 13, "max_num_pag": 13, "num_head": [13, 26, 27, 30, 32], "head_dim": 13, "num_group": 13, "pagedblockdiagonalcausalwithoffsetpaddedkeysmask": 13, "blockdiagonalgappykeysmask": 13, "_gappyseqinfo": 13, "gappi": 13, "kv_seqstart": 13, "blockdiagonalcausalwithoffsetgappykeysmask": 13, "pattern": [13, 26, 30], "band": 13, "its": 13, "further": 13, "memory_efficient_attention_backward": 13, "grad": 13, "output": [13, 29, 30], "lse": 13, "gradient": 13, "dq": 13, "dk": 13, "dv": 13, "explan": 13, "memory_efficient_attention_forward_requires_grad": 13, "memory_efficient_attention_forward": 13, "backward": 13, "later": 13, "transpar": 17, "implement": [17, 29], "sputnik": 17, "These": [17, 31], "instal": 17, "recipi": 17, "machin": 17, "abl": 17, "compil": [17, 31], "git": 17, "clone": 17, "github": 17, "fairintern": 17, "conda": 17, "name": [17, 27, 30, 32], "xformer_env": 17, "python": [17, 31], "activ": [17, 29], "cd": 17, "pip": 17, "r": [17, 29], "txt": 17, "issu": 17, "relat": [17, 29], "nvcc": 17, "current": [17, 31], "runtim": 17, "match": [17, 26], "often": 17, "chang": [17, 26], "modul": [17, 27, 29, 30], "unload": 17, "load": 17, "xx": 17, "version": 17, "gcc": 17, "re": [17, 30, 32], "torch_cuda_arch_list": 17, "env": 17, "variabl": 17, "architur": 17, "suggest": 17, "setup": 17, "comprehens": 17, "export": 17, "7": 17, "trigger": [17, 27], "dot": 17, "product": 17, "enough": [17, 26], "30": 17, "true": [17, 26, 27, 29, 31], "There": [17, 27, 31, 32], "noth": 17, "specif": [17, 33], "do": [17, 27, 30, 32], "coupl": [17, 27, 31], "tutori": [17, 31], "visibl": 17, "enabl": [17, 27, 31], "condit": 17, "met": 17, "warn": 17, "independ": 17, "model": [17, 18, 28, 29, 32, 33], "limit": [17, 26, 31], "present": 17, "fullfil": 17, "librari": [18, 33], "host": [18, 28], "flexibl": [18, 33], "transform": [18, 30, 31, 33], "interoper": [18, 33], "build": [18, 26, 29, 32, 33], "state": [18, 33], "art": [18, 33], "api": [18, 32], "refer": [18, 30, 33], "replac": [18, 28, 31], "exist": [18, 28], "vit": [18, 28], "spars": [18, 26, 28], "blocksparseattent": [18, 28], "extend": [18, 28, 33], "zoo": [18, 28, 33], "interest": [18, 28], "blockspars": 26, "tile": 26, "construct": [26, 27], "time": [26, 31], "simpl": 26, "just": [26, 30, 31], "minimum": 26, "being": [26, 31, 33], "coeffici": 26, "alreadi": [26, 30], "mind": [26, 30], "perfect": 26, "fine": [26, 32], "drop": [26, 31], "after": 26, "fact": 26, "grain": 26, "still": 26, "dens": 26, "helper": [26, 29, 30, 32], "maxpool": 26, "convert": 26, "binari": 26, "layout": 26, "now": [26, 27, 30], "power": 26, "two": [26, 29, 31, 32], "let": [26, 27, 30], "look": 26, "compon": [26, 27, 29, 30, 32], "multiheaddispatch": [26, 32], "seq": [26, 32], "2048": 26, "emb": 26, "1024": [26, 32], "block_siz": 26, "try": 26, "realli": [26, 33], "could": 26, "anyth": [26, 33], "causal_mask": 26, "tril": 26, "ones": 26, "causal_layout": 26, "our": [26, 30], "_you": 26, "head_": 26, "commod": 26, "multihead": [26, 32], "multi_head": [26, 32], "respons": 26, "seq_len": [26, 32], "dim_model": [26, 32], "residual_dropout": [26, 32], "half": 26, "fw": [26, 29], "random": 26, "data": [26, 32], "remov": 26, "blockif": 26, "requires_grad": 26, "particular": [26, 30], "att_val": 26, "att_mask": [26, 30], "bonu": 26, "compar": [26, 29, 31, 33], "vs": 26, "def": [26, 27, 29, 30], "mem_us": 26, "fn": 26, "titl": 26, "bookeep": 26, "empty_cach": 26, "reset_peak_memory_stat": 26, "actual": [26, 31], "synchron": 26, "stop": 26, "report": 26, "max_memori": 26, "max_memory_alloc": 26, "20": 26, "peak": 26, "mb": 26, "round": 26, "1e6": 26, "1e3": 26, "ms": 26, "pytorch_multihead": 26, "multiheadattent": 26, "batch_first": 26, "attn_mask": [26, 30], "On": 26, "v100": [26, 31], "9": 26, "someth": [26, 27, 29, 32], "line": [26, 31], "151mb": 26, "619m": 26, "393mb": 26, "837m": 26, "more": [26, 27, 29, 30, 32, 33], "get": [26, 29, 30, 31], "bias": 26, "result": [26, 31], "toward": 26, "privat": 27, "progress": 27, "would": [27, 30, 32], "share": 27, "point": [27, 31], "directli": [27, 31, 32], "order": 27, "submit": 27, "pr": [27, 33], "architectur": [27, 32, 33], "practic": [27, 30], "unit": [27, 31], "loos": 27, "inherit": 27, "expos": [27, 29, 30, 32], "exact": 27, "interfac": [27, 31], "consid": [27, 29, 30], "instanc": 27, "nystrom": 27, "dataclass": 27, "nystromselfattentionconfig": 27, "attentionconfig": 27, "register_attent": 27, "nystromattent": 27, "__init__": [27, 29], "num_landmark": 27, "64": 27, "landmark_pool": 27, "bool": [27, 29], "fals": [27, 29, 30, 31], "use_razavi_pinvers": 27, "pinverse_original_init": 27, "inv_iter": 27, "paper": [27, 29], "wa": [27, 29, 32], "v_skip_connect": 27, "conv_kernel_s": 27, "arg": 27, "remark": 27, "extens": [27, 33], "configur": 27, "explicitli": 27, "constructor": 27, "It": [27, 31], "benchmark": [27, 31, 33], "accept": 27, "even": 27, "field": [27, 33], "registr": 27, "snippet": 27, "ti": 27, "open": [27, 29], "up": [27, 29], "least": 27, "tool": 27, "toolbox": 27, "relev": [27, 33], "pick": 27, "variant": [27, 33], "them": [27, 29, 31], "go": 27, "pytest": 27, "my_component_nam": 27, "applic": [27, 29, 31], "lra": 27, "json": 27, "config": 27, "your": [27, 29, 32], "job": 27, "As": [27, 31], "remind": 27, "inform": 27, "dedic": 27, "readm": 27, "python3": [27, 31], "run_task": 27, "py": [27, 31], "task": 27, "config_path": 27, "world_siz": 27, "slurm": 27, "cluster": 27, "batch_submit": 27, "c": 27, "checkpo": 27, "log": [27, 31], "path": [27, 29, 32], "residu": 29, "propos": [29, 30, 31, 33], "gomez": 29, "et": 29, "al": 29, "Its": 29, "context": 29, "reform": 29, "unrel": 29, "lsh": 29, "chunk": 29, "mlp": 29, "process": 29, "lightli": 29, "adapt": 29, "robin": 29, "bruegger": 29, "lucidrain": 29, "x1": 29, "x2": 29, "produc": 29, "y1": 29, "y2": 29, "turn": [29, 32], "mean": [29, 33], "recov": 29, "detail": 29, "anoth": 29, "effect": 29, "checkpoint": 29, "tradeoff": 29, "One": 29, "benefit": 29, "wrap": 29, "natur": 29, "distribut": 29, "free": 29, "help": 29, "save": 29, "commun": 29, "cost": 29, "moreov": 29, "made": 29, "stack": 29, "increas": [29, 31], "norm": 29, "close": 29, "origin": 29, "formul": 29, "vaswani": 29, "deal": 29, "accuraci": 29, "affect": 29, "verifi": 29, "repositori": [29, 30], "main": 29, "reversibleblock": 29, "reversiblesequ": 29, "take": [29, 30, 32], "sequenti": 29, "similarli": 29, "modulelist": 29, "f_arg": 29, "g_arg": 29, "arg_rout": 29, "whether": [29, 32], "rout": 29, "boolean": 29, "complet": [29, 32], "factori": 29, "model_factori": 29, "yet": 29, "compat": [29, 31], "multipl": 29, "ddp": 29, "xformerstackconfig": 29, "block_config": 29, "xformerencoderconfig": 29, "xformerdecoderconfig": 29, "num_lay": 29, "ren": 29, "urtasun": 29, "gross": 29, "2017": 29, "network": 29, "backpropag": 29, "store": 29, "kitaev": 29, "kaiser": 29, "\u0142": 29, "levskaya": 29, "2020": 29, "sai": 30, "experi": 30, "show": 30, "how": [30, 31], "reus": [30, 33], "imag": 30, "aspect": 30, "translat": 30, "well": 30, "check": 30, "notebook": 30, "exhaust": 30, "timm": 30, "vision_transform": 30, "visiontransform": 30, "scaleddotproduct": 30, "timm_sparse_attent": 30, "timmsparseattent": 30, "img_siz": 30, "224": 30, "patch_siz": 30, "embed_dim": 30, "96": 30, "depth": 30, "mlp_ratio": 30, "qkv_bia": 30, "norm_lay": 30, "layernorm": 30, "suppos": 30, "snipper": 30, "precis": 30, "sever": 30, "attention_pattern": 30, "my_fancy_mask": 30, "recurs": 30, "monkei": 30, "patch": 30, "replace_attn_with_xformers_on": 30, "module_output": 30, "isinst": 30, "qkv": 30, "weight": 30, "minim": 30, "child": 30, "named_children": 30, "add_modul": 30, "del": 30, "awar": [30, 31], "variat": 30, "exchang": 30, "mai": 30, "good": 30, "idea": 30, "closer": 30, "typic": [30, 31], "exhibit": 30, "clear": 30, "sparsiti": 30, "alter": 30, "sparsifi": 30, "languag": 31, "parallel": 31, "program": 31, "pure": 31, "mani": 31, "primit": 31, "tranform": 31, "backend": 31, "short": 31, "jit": 31, "toolchain": 31, "famili": 31, "consolid": 31, "ad": 31, "hoc": 31, "goal": 31, "over": 31, "avail": 31, "similar": [31, 32], "http": 31, "lang": 31, "org": 31, "02": 31, "html": 31, "sphx": 31, "glr": 31, "log_softmax": 31, "amp": 31, "autograd": 31, "expect": [31, 32], "throughput": 31, "operand": 31, "relu": 31, "simpli": 31, "fusedlinearlay": 31, "my_linear_lay": 31, "in_featur": 31, "out_featur": 31, "squared_relu": 31, "skip": 31, "septemb": 31, "2021": 31, "faster": 31, "non": 31, "sigmoid": 31, "fp16": 31, "usecas": 31, "serv": 31, "measur": [31, 33], "laptop": 31, "3080": 31, "10": 31, "reproduc": 31, "benchmark_triton_layernorm": 31, "gb": 31, "benchmark_triton_dropout": 31, "own": 32, "everyth": 32, "requires_head_dimens": 32, "flag": 32, "build_attent": 32, "defer": 32, "lot": 32, "instanti": 32, "littl": 32, "obscur": 32, "although": 32, "hopefulli": 32, "straightforward": 32, "built": 32, "intern": 32, "sure": 32, "programat": 32, "sweep": 32, "search": [32, 33], "definit": 32, "384": 32, "my_config": 32, "attention_nam": 32, "easili": [32, 33], "file": 32, "attention_query_mask": 32, "rand": 32, "dummi": 32, "my": 32, "focus": 33, "agnost": 33, "design": 33, "ideal": 33, "break": 33, "inspir": 33, "studi": 33, "ablat": 33, "aim": 33, "easi": 33, "focu": 33, "improv": 33, "against": 33, "across": 33, "domain": 33, "engin": 33, "effort": 33, "And": 33, "sinc": 33, "heavi": 33, "everi": 33, "repo": 33, "alon": 33, "happen": 33, "anytim": 33, "somebodi": 33, "through": 33, "crowd": 33, "welcom": 33, "move": 33, "too": 33}, "objects": {"xformers": [[13, 0, 0, "-", "ops"], [17, 0, 0, "-", "triton"]], "xformers.ops": [[13, 1, 1, "", "AttentionBias"], [13, 1, 1, "", "AttentionOpBase"], [13, 0, 0, "-", "fmha"], [13, 3, 1, "", "memory_efficient_attention"]], "xformers.ops.AttentionBias": [[13, 2, 1, "", "materialize"]], "xformers.ops.AttentionOpBase": [[13, 2, 1, "", "not_supported_reasons"]], "xformers.ops.fmha": [[13, 0, 0, "-", "attn_bias"], [13, 0, 0, "-", "ck"], [13, 0, 0, "-", "ck_decoder"], [13, 0, 0, "-", "ck_splitk"], [13, 0, 0, "-", "cutlass"], [13, 0, 0, "-", "flash"], [13, 3, 1, "", "memory_efficient_attention_backward"], [13, 3, 1, "", "memory_efficient_attention_forward"], [13, 3, 1, "", "memory_efficient_attention_forward_requires_grad"], [13, 0, 0, "-", "small_k"]], "xformers.ops.fmha.attn_bias": [[13, 1, 1, "", "AttentionBias"], [13, 1, 1, "", "BlockDiagonalCausalFromBottomRightMask"], [13, 1, 1, "", "BlockDiagonalCausalLocalAttentionFromBottomRightMask"], [13, 1, 1, "", "BlockDiagonalCausalLocalAttentionMask"], [13, 1, 1, "", "BlockDiagonalCausalMask"], [13, 1, 1, "", "BlockDiagonalCausalWithOffsetGappyKeysMask"], [13, 1, 1, "", "BlockDiagonalCausalWithOffsetPaddedKeysMask"], [13, 1, 1, "", "BlockDiagonalGappyKeysMask"], [13, 1, 1, "", "BlockDiagonalMask"], [13, 1, 1, "", "BlockDiagonalPaddedKeysMask"], [13, 1, 1, "", "LocalAttentionFromBottomRightMask"], [13, 1, 1, "", "LowerTriangularFromBottomRightLocalAttentionMask"], [13, 1, 1, "", "LowerTriangularFromBottomRightMask"], [13, 1, 1, "", "LowerTriangularMask"], [13, 1, 1, "", "LowerTriangularMaskWithTensorBias"], [13, 1, 1, "", "PagedBlockDiagonalCausalWithOffsetPaddedKeysMask"], [13, 1, 1, "", "PagedBlockDiagonalPaddedKeysMask"]], "xformers.ops.fmha.attn_bias.AttentionBias": [[13, 2, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetGappyKeysMask": [[13, 2, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask": [[13, 2, 1, "", "from_seqlens"], [13, 2, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalGappyKeysMask": [[13, 2, 1, "", "from_seqlens"], [13, 2, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalMask": [[13, 2, 1, "", "from_seqlens"], [13, 2, 1, "", "from_tensor_list"], [13, 2, 1, "", "make_causal"], [13, 2, 1, "", "make_causal_from_bottomright"], [13, 2, 1, "", "make_local_attention"], [13, 2, 1, "", "make_local_attention_from_bottomright"], [13, 2, 1, "", "materialize"], [13, 2, 1, "", "split"]], "xformers.ops.fmha.attn_bias.BlockDiagonalPaddedKeysMask": [[13, 2, 1, "", "from_seqlens"], [13, 2, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.LowerTriangularFromBottomRightMask": [[13, 2, 1, "", "make_local_attention"]], "xformers.ops.fmha.attn_bias.LowerTriangularMask": [[13, 2, 1, "", "add_bias"]], "xformers.ops.fmha.attn_bias.PagedBlockDiagonalPaddedKeysMask": [[13, 2, 1, "", "from_seqlens"], [13, 2, 1, "", "materialize"]], "xformers.ops.fmha.ck": [[13, 1, 1, "", "BwOp"], [13, 1, 1, "", "FwOp"]], "xformers.ops.fmha.ck_decoder": [[13, 1, 1, "", "FwOp"]], "xformers.ops.fmha.cutlass": [[13, 1, 1, "", "BwOp"], [13, 1, 1, "", "FwOp"]], "xformers.ops.fmha.flash": [[13, 1, 1, "", "BwOp"], [13, 1, 1, "", "FwOp"]], "xformers.ops.fmha.small_k": [[13, 1, 1, "", "BwOp"], [13, 1, 1, "", "FwOp"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"]}, "titleterms": {"attent": [9, 12, 13, 30, 32], "mechan": [9, 10, 32], "feedforward": 10, "api": 11, "refer": [11, 17], "multi": 12, "head": 12, "xformer": [13, 18, 27, 33], "optim": 13, "oper": 13, "memori": 13, "effici": 13, "avail": 13, "implement": 13, "bias": 13, "non": 13, "autograd": 13, "posit": 14, "embed": 14, "revers": [15, 29], "layer": [15, 31], "custom": [17, 18], "part": [17, 18, 27], "spars": [17, 30], "cuda": 17, "kernel": 17, "1": 17, "build": 17, "2": 17, "usag": 17, "triton": [17, 31], "requir": 17, "possibl": 17, "welcom": 18, "s": 18, "document": 18, "compon": 18, "tutori": [18, 28], "exampl": 18, "some": 18, "us": [26, 29, 31], "blocksparseattent": 26, "extend": 27, "zoo": 27, "block": 29, "intro": 29, "transform": 29, "In": 29, "practic": 29, "replac": 30, "all": 30, "from": 30, "an": 30, "exist": 30, "vit": 30, "model": 30, "equival": 30, "base": 31, "fuse": 31, "softmax": 31, "linear": 31, "norm": 31, "dropout": 31, "bia": 31, "activ": 31, "i": 32, "m": 32, "onli": 32, "interest": 32, "test": 32, "out": 32, "ar": 32, "host": 32, "here": 32, "what": 33}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})